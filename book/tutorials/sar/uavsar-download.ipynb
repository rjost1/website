{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "statutory-candidate",
   "metadata": {},
   "source": [
    "# UAVSAR Data Access and Conversion\n",
    "\n",
    "```{admonition} Learning Objectives\n",
    "- overview of UAVSAR data (both InSAR and PolSAR products)\n",
    "- demonstrate how to access and transform data to Geotiffs\n",
    "```\n",
    "\n",
    "There are multiple ways to access UAVSAR data. Also the SQL database.\n",
    "\n",
    "* [Alaska Satellite Facility Vertex Portal](https://search.asf.alaska.edu/#/?dataset=UAVSAR)\n",
    "* [NASA Earthdata Suborbital Search](https://search.earthdata.nasa.gov/portal/suborbital/search?fi=UAVSAR&as[instrument][0]=UAVSAR)\n",
    "* [JPL UAVSAR Data Search](https://uavsar.jpl.nasa.gov/cgi-bin/data.pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-craps",
   "metadata": {},
   "source": [
    "```{admonition} InSAR Data Types\n",
    ":class: InSAR Data Types\n",
    "- ANN file (.ann): a text annotation file with metadata\n",
    "- AMP files (.amp1 and .amp2): calibrated multi-looked amplitude products\n",
    "- INT files (.int): interferogram product, complex number format (we won't be using these here)\n",
    "- COR files (.cor): interferometric correlation product, a measure of the noise level of the phase\n",
    "- GRD files (.grd): interferometric products projected to the ground in simple geographic coordinates (latitude, longitude)\n",
    "- HGT file  (.hgt): the DEM that was used in the InSAR processing\n",
    "- KML and KMZ files (.kml or .kmz): format for viewing files in Google Earth (can't be used for analysis)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cosmetic-chamber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import re\n",
    "import zipfile\n",
    "import getpass\n",
    "from osgeo import gdal \n",
    "import os  # for chdir, getcwd, path.basename, path.exists\n",
    "import pandas as pd # for DatetimeIndex\n",
    "import codecs # for text parsing code\n",
    "import netrc\n",
    "import rasterio as rio\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-repeat",
   "metadata": {},
   "source": [
    "### Data Download\n",
    "\n",
    "We will use our NASA EarthData credentials and ASF Vertex to download an InSAR pair data into our notebook directly. For this tutorial, we will be working with UAVSAR data from February of 2020. If you want to use different data in the future, change the links in the files variable. The screengrab below shows how I generated these download links from the ASF site."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-print",
   "metadata": {},
   "source": [
    ":::{figure-md} vertex\n",
    "<img src=\"../../img/asf_vertex.png\" alt=\"asf vertex\" width=\"800px\">\n",
    "\n",
    "Screenshot of ASF Vertex interface\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "organic-print",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Username:  morageology\n",
      "Enter Password:  ··········\n"
     ]
    }
   ],
   "source": [
    "# Get NASA EARTHDATA Credentials from ~/.netrc or manual input\n",
    "try:\n",
    "    os.chmod('/home/jovyan/.netrc', 0o600) #only necessary on jupyterhub\n",
    "    (ASF_USER, account, ASF_PASS) = netrc.netrc().authenticators(\"urs.earthdata.nasa.gov\")\n",
    "except:\n",
    "    ASF_USER = input(\"Enter Username: \")\n",
    "    ASF_PASS = getpass.getpass(\"Enter Password: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "impressive-space",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook directory:  /home/jovyan/website/book/tutorials/sar\n",
      "/tmp\n"
     ]
    }
   ],
   "source": [
    "# directory in which the notebook resides\n",
    "if 'tutorial_home_dir' not in globals():\n",
    "     tutorial_home_dir = os.getcwd()\n",
    "print(\"Notebook directory: \", tutorial_home_dir)\n",
    "\n",
    "if not os.path.exists('/tmp/'):\n",
    "    os.chdir('/tmp')\n",
    "   \n",
    "# directory for data downloads\n",
    "\n",
    "data_dir = os.path.join('/tmp')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "banned-breast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/irnton_01406_21016-003_21018-003_0007d_s01_L090_01_int_grd.zip...\n",
      "downloading https://datapool.asf.alaska.edu/AMPLITUDE_GRD/UA/irnton_01406_21020-028_21022-002_0006d_s01_L090_01_amp_grd.zip...\n",
      "done\n",
      "CPU times: user 12.1 ms, sys: 8.1 ms, total: 20.3 ms\n",
      "Wall time: 3min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "files = ['https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/irnton_01406_21016-003_21018-003_0007d_s01_L090_01_int_grd.zip',\n",
    "        'https://datapool.asf.alaska.edu/AMPLITUDE_GRD/UA/irnton_01406_21020-028_21022-002_0006d_s01_L090_01_amp_grd.zip']\n",
    "    \n",
    "for file in files:\n",
    "    print(f'downloading {file}...')\n",
    "    filename = os.path.basename(file)\n",
    "    \n",
    "    if not os.path.exists(os.path.join(data_dir,filename)):\n",
    "        cmd = \"wget -q {0} --user={1} --password={2} -P {3} -nc\".format(file, ASF_USER, ASF_PASS, data_dir)\n",
    "        os.system(cmd)\n",
    "    else:\n",
    "        print(filename + \" already exists. Skipping download ..\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "german-python",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/tmp/irnton_01406_21016-003_21018-003_0007d_s01_L090_01_int_grd.zip', '/tmp/irnton_01406_21020-028_21022-002_0006d_s01_L090_01_amp_grd.zip']\n"
     ]
    }
   ],
   "source": [
    "# check to see if downloaded\n",
    "# the *.* syntax means print all files in the directory\n",
    "\n",
    "print(glob.glob(\"/tmp/*.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use for deleting contents of temp directory if needed. DO NOT uncomment, this will cause the notebook to fail.\n",
    "\n",
    "#files = glob.glob(\"/tmp/*.*\")\n",
    "#for f in files:\n",
    "#    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-navigation",
   "metadata": {},
   "source": [
    "### Unzipping the files we just downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "elegant-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "## unzip files just downloaded\n",
    "\n",
    "# define file path for both files\n",
    "int_zip = '/tmp/irnton_01406_21016-003_21018-003_0007d_s01_L090_01_int_grd.zip'\n",
    "amp_zip = '/tmp/irnton_01406_21020-028_21022-002_0006d_s01_L090_01_amp_grd.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "incorporated-antarctica",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name                                             Modified             Size\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090HH_01.cor.grd 2021-04-21 00:26:46    217161860\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090HH_01.hgt.grd 2021-04-21 00:32:20    217161860\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090HH_01.int.grd 2021-04-21 00:32:30    434323720\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090HH_01.unw.grd 2021-04-21 00:32:46    217161860\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090HV_01.cor.grd 2021-04-21 00:32:52    217161860\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090HV_01.hgt.grd 2021-04-21 00:33:02    217161860\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090HV_01.int.grd 2021-04-21 00:33:12    434323720\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090HV_01.unw.grd 2021-04-21 00:33:28    217161860\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090VH_01.cor.grd 2021-04-21 00:33:34    217161860\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090VH_01.hgt.grd 2021-04-21 00:33:44    217161860\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090VH_01.int.grd 2021-04-21 00:33:54    434323720\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090VH_01.unw.grd 2021-04-21 00:34:10    217161860\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090VV_01.cor.grd 2021-04-21 00:34:16    217161860\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090VV_01.hgt.grd 2021-04-21 00:34:26    217161860\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090VV_01.int.grd 2021-04-21 00:34:36    434323720\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090VV_01.unw.grd 2021-04-21 00:34:52    217161860\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090VH_01.ann 2021-04-21 00:34:58        29891\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090VV_01.ann 2021-04-21 00:34:58        29891\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090HV_01.ann 2021-04-21 00:34:58        29891\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090HH_01.ann 2021-04-21 00:34:58        29891\n",
      "Extracting all the files now...\n",
      "done\n",
      "File Name                                             Modified             Size\n",
      "irnton_01406_21020-028_21022-002_0006d_s01_L090HH_01.amp1.grd 2021-04-24 06:00:54    216613908\n",
      "irnton_01406_21020-028_21022-002_0006d_s01_L090HH_01.amp2.grd 2021-04-24 06:02:44    216613908\n",
      "irnton_01406_21020-028_21022-002_0006d_s01_L090HH_01.hgt.grd 2021-04-24 06:02:54    216613908\n",
      "irnton_01406_21020-028_21022-002_0006d_s01_L090HV_01.amp1.grd 2021-04-24 06:03:04    216613908\n",
      "irnton_01406_21020-028_21022-002_0006d_s01_L090HV_01.amp2.grd 2021-04-24 06:03:12    216613908\n",
      "irnton_01406_21020-028_21022-002_0006d_s01_L090HV_01.hgt.grd 2021-04-24 06:03:22    216613908\n",
      "irnton_01406_21020-028_21022-002_0006d_s01_L090VH_01.amp1.grd 2021-04-24 06:03:32    216613908\n",
      "irnton_01406_21020-028_21022-002_0006d_s01_L090VH_01.amp2.grd 2021-04-24 06:03:42    216613908\n",
      "irnton_01406_21020-028_21022-002_0006d_s01_L090VH_01.hgt.grd 2021-04-24 06:03:50    216613908\n",
      "irnton_01406_21020-028_21022-002_0006d_s01_L090VV_01.amp1.grd 2021-04-24 06:04:00    216613908\n",
      "irnton_01406_21020-028_21022-002_0006d_s01_L090VV_01.amp2.grd 2021-04-24 06:04:10    216613908\n",
      "irnton_01406_21020-028_21022-002_0006d_s01_L090VV_01.hgt.grd 2021-04-24 06:04:18    216613908\n",
      "irnton_01406_21020-028_21022-002_0006d_s01_L090VV_01.ann 2021-04-24 06:04:28        29890\n",
      "irnton_01406_21020-028_21022-002_0006d_s01_L090HH_01.ann 2021-04-24 06:04:28        29890\n",
      "irnton_01406_21020-028_21022-002_0006d_s01_L090HV_01.ann 2021-04-24 06:04:28        29890\n",
      "irnton_01406_21020-028_21022-002_0006d_s01_L090VH_01.ann 2021-04-24 06:04:28        29890\n",
      "Extracting all the files now...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# unzip\n",
    "\n",
    "# int\n",
    "with zipfile.ZipFile(int_zip, \"r\") as zip_ref:\n",
    "    zip_ref.printdir()\n",
    "    print('Extracting all the files now...')\n",
    "    zip_ref.extractall('/tmp')\n",
    "    print(\"done\")\n",
    "    \n",
    "# amp\n",
    "with zipfile.ZipFile(amp_zip, \"r\") as zip_ref:\n",
    "    zip_ref.printdir()\n",
    "    print('Extracting all the files now...')\n",
    "    zip_ref.extractall('/tmp')\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-entry",
   "metadata": {},
   "source": [
    "### Removing unwanted data\n",
    "For simplicity, we'll only work with HH polarization. The three other polarizations (VV, VH, HV) provide additional information about the surface properties and can be utilized in further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "qualified-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up unwanted data from what we just downloaded\n",
    "\n",
    "directory = '/tmp'\n",
    "os.chdir(directory)\n",
    "HV_files = glob.glob('*HV_01*') #define all HV\n",
    "VV_files = glob.glob('*VV_01*') #define all VV\n",
    "VH_files = glob.glob('*VH_01*') #define all VH\n",
    "zips = glob.glob('*.zip') # define the zip files\n",
    "\n",
    "# loops to remove them\n",
    "\n",
    "for f in HV_files:\n",
    "    os.remove(f)\n",
    "    \n",
    "for f in VV_files:\n",
    "    os.remove(f)\n",
    "    \n",
    "for f in VH_files:\n",
    "    os.remove(f)\n",
    "    \n",
    "for f in zips:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "adverse-anatomy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/tmp/irnton_01406_21016-003_21018-003_0007d_s01_L090HH_01.cor.grd', '/tmp/irnton_01406_21016-003_21018-003_0007d_s01_L090HH_01.hgt.grd', '/tmp/irnton_01406_21016-003_21018-003_0007d_s01_L090HH_01.int.grd', '/tmp/irnton_01406_21016-003_21018-003_0007d_s01_L090HH_01.unw.grd', '/tmp/irnton_01406_21016-003_21018-003_0007d_s01_L090HH_01.ann', '/tmp/irnton_01406_21020-028_21022-002_0006d_s01_L090HH_01.amp1.grd', '/tmp/irnton_01406_21020-028_21022-002_0006d_s01_L090HH_01.amp2.grd', '/tmp/irnton_01406_21020-028_21022-002_0006d_s01_L090HH_01.hgt.grd', '/tmp/irnton_01406_21020-028_21022-002_0006d_s01_L090HH_01.ann']\n"
     ]
    }
   ],
   "source": [
    "# check to see what files are left in the directory\n",
    "\n",
    "print(glob.glob(\"/tmp/*.*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-comparison",
   "metadata": {},
   "source": [
    "Now we only have the HH polarization, the annotation file, and the 6 .grd files!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-probe",
   "metadata": {},
   "source": [
    "## Converting Data to GeoTiffs\n",
    "\n",
    "The downloadable UAVSAR data comes in a flat binary format (.grd), which is not readable by GDAL (Geospatial Data Abstraction Library). Therefore it needs to be transformed for use in standard spatial analysis software (ArcGIS, QGIS, Python, R, MATLAB, etc.). To do this, we will use the uavsar_tiff_convert function, which takes information (latitude, longitude, number of lines and samples, data type, pixel size) from the annotation file to create an ENVI header (.hdr). Once the ENVI header is created, the files can be read into Python and converted to GeoTiffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "square-valuable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "; UAVSAR RPI Metadata file for irnton_01406_21016-003_21018-003_0007d_s01_L090HH_01\n",
      "; search for parameters/value rather than placement in file\n",
      "\n",
      "; Annotation file version (when an annotation file contains no annotation version field, assume the version number is 1)\n",
      "UAVSAR RPI Annotation File Version Number      (-)             = 2.3\n",
      "\n",
      "; Geographic location of data (non-unique)\n",
      "Site Description                               (&)             = Ironton, CO\n",
      "\n",
      "; Comments\n",
      "Flight Plan Comments of Pass 1                 (&)             = N/A\n",
      "Flight Plan Comments of Pass 2                 (&)             = N/A\n",
      "Processing Comments                            (&)             = N/A\n",
      "\n",
      "; URL of JPL website for precision data\n"
     ]
    }
   ],
   "source": [
    "# First, let's print the annotation file to get a look at it's content\n",
    "# these file contain a lot of information and can be very intimidating and hard to understand, but being able to read them is vital to working this UAVSAR data\n",
    "\n",
    "#f = open('/tmp/grmesa_27416_20003-028_20005-007_0011d_s01_L090HH_01.ann', 'r')\n",
    "#file_contents = f.read()\n",
    "#print (file_contents)\n",
    "!head -n 15 /tmp/irnton_01406_21016-003_21018-003_0007d_s01_L090HH_01.ann"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-northwest",
   "metadata": {},
   "source": [
    "This function pulls out information from the annotation file, builds and ENVI header, and then converts the data to GeoTIFFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "freelance-development",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder is path to a folder with an .ann (or .txt) and .grd files (.amp1, .amp2, .cor, .unw, .int)\n",
    "\n",
    "def uavsar_tiff_convert(folder):\n",
    "    \"\"\"\n",
    "    Builds a header file for the input UAVSAR .grd file,\n",
    "    allowing the data to be read as a raster dataset.\n",
    "    :param folder:   the folder containing the UAVSAR .grd and .ann files\n",
    "    \"\"\"\n",
    "\n",
    "    os.chdir(folder)\n",
    "    int_file = glob.glob(os.path.join(folder, 'int.grd'))\n",
    "\n",
    "    # Empty lists to put information that will be recalled later.\n",
    "    Lines_list = []\n",
    "    Samples_list = []\n",
    "    Latitude_list = []\n",
    "    Longitude_list = []\n",
    "    Files_list = []\n",
    "\n",
    "    # Step 1: Look through folder and determine how many different flights there are\n",
    "    # by looking at the HDR files.\n",
    "    for files in os.listdir(folder):\n",
    "        if files [-4:] == \".grd\":\n",
    "            newfile = open(files[0:-4] + \".hdr\", 'w')\n",
    "            newfile.write(\"\"\"ENVI\n",
    "description = {DESCFIELD}\n",
    "samples = NSAMP\n",
    "lines = NLINE\n",
    "bands = 1\n",
    "header offset = 0\n",
    "data type = DATTYPE\n",
    "interleave = bsq\n",
    "sensor type = UAVSAR L-Band\n",
    "byte order = 0\n",
    "map info = {Geographic Lat/Lon, \n",
    "            1.000, \n",
    "            1.000, \n",
    "            LON, \n",
    "            LAT,  \n",
    "            0.0000555600000000, \n",
    "            0.0000555600000000, \n",
    "            WGS-84, units=Degrees}\n",
    "wavelength units = Unknown\n",
    "                \"\"\"\n",
    "                          )\n",
    "            newfile.close()\n",
    "            if files[0:18] not in Files_list:\n",
    "                Files_list.append(files[0:18])\n",
    "\n",
    "    #Variables used to recall indexed values.\n",
    "    var1 = 0\n",
    "\n",
    "    #Step 2: Look through the folder and locate the annotation file(s).\n",
    "    # These can be in either .txt or .ann file types.\n",
    "    for files in os.listdir(folder):\n",
    "        if Files_list[var1] and files[-4:] == \".txt\" or files[-4:] == \".ann\":\n",
    "            #Step 3: Once located, find the info we are interested in and append it to\n",
    "            # the appropriate list. We limit the variables to <=1 so that they only\n",
    "            # return two values (one for each polarization of\n",
    "            searchfile = codecs.open(files, encoding = 'windows-1252', errors='ignore')\n",
    "            for line in searchfile:\n",
    "                if \"Ground Range Data Latitude Lines\" in line:\n",
    "                    Lines = line[65:70]\n",
    "                    print(f\"Number of Lines: {Lines}\")\n",
    "                    if Lines not in Lines_list:\n",
    "                        Lines_list.append(Lines)\n",
    "\n",
    "                elif \"Ground Range Data Longitude Samples\" in line:\n",
    "                    Samples = line[65:70]\n",
    "                    print(f\"Number of Samples: {Samples}\")\n",
    "                    if Samples not in Samples_list:\n",
    "                        Samples_list.append(Samples)\n",
    "\n",
    "                elif \"Ground Range Data Starting Latitude\" in line:\n",
    "                    Latitude = line[65:85]\n",
    "                    print(f\"Top left lat: {Latitude}\")\n",
    "                    if Latitude not in Latitude_list:\n",
    "                        Latitude_list.append(Latitude)\n",
    "\n",
    "                elif \"Ground Range Data Starting Longitude\" in line:\n",
    "                    Longitude = line[65:85]\n",
    "                    print(f\"Top left Lon: {Longitude}\")\n",
    "                    if Longitude not in Longitude_list:\n",
    "                        Longitude_list.append(Longitude)\n",
    "    \n",
    "                        \n",
    "                 \n",
    "            #Reset the variables to zero for each different flight date.\n",
    "            var1 = 0\n",
    "            searchfile.close()\n",
    "\n",
    "\n",
    "    # Step 3: Open .hdr file and replace data for all type 4 (real numbers) data\n",
    "    # this all the .grd files expect for .int\n",
    "    for files in os.listdir(folder):\n",
    "        if files[-4:] == \".hdr\":\n",
    "            with open(files, \"r\") as sources:\n",
    "                lines = sources.readlines()\n",
    "            with open(files, \"w\") as sources:\n",
    "                for line in lines:\n",
    "                    if \"data type = DATTYPE\" in line:\n",
    "                        sources.write(re.sub(line[12:19], \"4\", line))\n",
    "                    elif \"DESCFIELD\" in line:\n",
    "                        sources.write(re.sub(line[15:24], folder, line))\n",
    "                    elif \"lines\" in line:\n",
    "                        sources.write(re.sub(line[8:13], Lines, line))\n",
    "                    elif \"samples\" in line:\n",
    "                        sources.write(re.sub(line[10:15], Samples, line))\n",
    "                    elif \"LAT\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Latitude, line))\n",
    "                    elif \"LON\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Longitude, line))\n",
    "                    else:\n",
    "                        sources.write(re.sub(line, line, line))\n",
    "    \n",
    "    # Step 3: Open .hdr file and replace data for .int file date type 6 (complex)                 \n",
    "    for files in os.listdir(folder):\n",
    "        if files[-8:] == \".int.hdr\":\n",
    "            with open(files, \"r\") as sources:\n",
    "                lines = sources.readlines()\n",
    "            with open(files, \"w\") as sources:\n",
    "                for line in lines:\n",
    "                    if \"data type = 4\" in line:\n",
    "                        sources.write(re.sub(line[12:13], \"6\", line))\n",
    "                    elif \"DESCFIELD\" in line:\n",
    "                        sources.write(re.sub(line[15:24], folder, line))\n",
    "                    elif \"lines\" in line:\n",
    "                        sources.write(re.sub(line[8:13], Lines, line))\n",
    "                    elif \"samples\" in line:\n",
    "                        sources.write(re.sub(line[10:15], Samples, line))\n",
    "                    elif \"LAT\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Latitude, line))\n",
    "                    elif \"LON\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Longitude, line))\n",
    "                    else:\n",
    "                        sources.write(re.sub(line, line, line))\n",
    "                        \n",
    "    \n",
    "    # Step 4: Now we have an .hdr file, the data is geocoded and can be loaded into python with rasterio\n",
    "    # once loaded in we use gdal.Translate to convert and save as a .tiff\n",
    "    \n",
    "    data_to_process = glob.glob(os.path.join(folder, '*.grd')) # list all .grd files\n",
    "    for data_path in data_to_process: # loop to open and translate .grd to .tiff, and save .tiffs using gdal\n",
    "        raster_dataset = gdal.Open(data_path, gdal.GA_ReadOnly)\n",
    "        raster = gdal.Translate(os.path.join(folder, os.path.basename(data_path) + '.tiff'), raster_dataset, format = 'Gtiff', outputType = gdal.GDT_Float32)\n",
    "    \n",
    "    # Step 5: Save the .int raster, needs separate save because of the complex format\n",
    "    data_to_process = glob.glob(os.path.join(folder, '*.int.grd')) # list all .int.grd files (only 1)\n",
    "    for data_path in data_to_process:\n",
    "        raster_dataset = gdal.Open(data_path, gdal.GA_ReadOnly)\n",
    "        raster = gdal.Translate(os.path.join(folder, os.path.basename(data_path) + '.tiff'), raster_dataset, format = 'Gtiff', outputType = gdal.GDT_CFloat32)\n",
    "\n",
    "    print(\".tiffs have been created\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "signed-specification",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/tmp' # define folder where the .grd and .ann files are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "junior-minute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Lines: 8395\n",
      "\n",
      "Number of Samples: 6467\n",
      "\n",
      "Top left lat: 38.126383199999999  \n",
      "Top left Lon: -107.914021320000003\n",
      "Number of Lines: 8379\n",
      "\n",
      "Number of Samples: 6463\n",
      "\n",
      "Top left lat: 38.124105240000006  \n",
      "Top left Lon: -107.914410240000009\n",
      ".tiffs have been created\n"
     ]
    }
   ],
   "source": [
    "uavsar_tiff_convert(data_folder) # call the tiff convert function, and it will print the information it extracted from the .ann file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-philip",
   "metadata": {},
   "source": [
    "Now we'll delete the unneeded .grd and .hdr files that our tiffs have been created. If you're using this code on your local machine, this probably isn't absolutely necessary. The JupyterHub cloud we're working in has limited space, so deletion is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "after-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(data_folder)\n",
    "grd = glob.glob('*.grd') #define .grd\n",
    "hdr = glob.glob('*.hdr*') #define .hdr\n",
    "\n",
    "# remove both\n",
    "for f in grd:\n",
    "    os.remove(f)\n",
    "    \n",
    "for f in hdr:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "egyptian-adult",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['irnton_01406_21016-003_21018-003_0007d_s01_L090HH_01.cor.grd.tiff', 'irnton_01406_21016-003_21018-003_0007d_s01_L090HH_01.unw.grd.tiff', 'irnton_01406_21020-028_21022-002_0006d_s01_L090HH_01.hgt.grd.tiff', 'irnton_01406_21016-003_21018-003_0007d_s01_L090HH_01.ann', 'irnton_01406_21020-028_21022-002_0006d_s01_L090HH_01.amp2.grd.tiff', 'irnton_01406_21020-028_21022-002_0006d_s01_L090HH_01.ann', 'irnton_01406_21016-003_21018-003_0007d_s01_L090HH_01.hgt.grd.tiff', 'irnton_01406_21020-028_21022-002_0006d_s01_L090HH_01.amp1.grd.tiff', 'irnton_01406_21016-003_21018-003_0007d_s01_L090HH_01.int.grd.tiff']\n"
     ]
    }
   ],
   "source": [
    "# check what's in the directory, only .tiffs and our annotation file!\n",
    "print(glob.glob(\"*.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "satisfied-september",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irnton_01406_21020-028_21022-002_0006d_s01_L090HH_01.amp1.grd.tiff\n",
      "irnton_01406_21020-028_21022-002_0006d_s01_L090HH_01.amp2.grd.tiff\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090HH_01.cor.grd.tiff\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090HH_01.unw.grd.tiff\n",
      "irnton_01406_21020-028_21022-002_0006d_s01_L090HH_01.hgt.grd.tiff\n",
      "irnton_01406_21016-003_21018-003_0007d_s01_L090HH_01.hgt.grd.tiff\n"
     ]
    }
   ],
   "source": [
    "### inspect our newly created .tiffs, and create named objects for each data type. We'll use these new obects in the next step\n",
    "\n",
    "# amplitude from the first acquisition\n",
    "for amp1 in glob.glob(\"*amp1.grd.tiff\"):\n",
    "    print(amp1)\n",
    "    \n",
    "# amplitude from the second acquisition\n",
    "for amp2 in glob.glob(\"*amp2.grd.tiff\"):\n",
    "    print(amp2)\n",
    "\n",
    "# coherence\n",
    "for cor in glob.glob(\"*cor.grd.tiff\"):\n",
    "    print(cor)\n",
    "\n",
    "# unwrapped phase\n",
    "for unw in glob.glob(\"*unw.grd.tiff\"):\n",
    "    print(unw)\n",
    "\n",
    "# dem used in processing\n",
    "for dem in glob.glob(\"*hgt.grd.tiff\"):\n",
    "    print(dem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-incident",
   "metadata": {},
   "source": [
    "Inspect the meta data the rasters using the rio (shorthand for rasterio) ```profile``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "color-textbook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'driver': 'GTiff', 'dtype': 'float32', 'nodata': None, 'width': 6463, 'height': 8379, 'count': 1, 'crs': CRS.from_epsg(4326), 'transform': Affine(5.556e-05, 0.0, -107.91441024000001,\n",
      "       0.0, -5.556e-05, 38.124105240000006), 'tiled': False, 'interleave': 'band'}\n"
     ]
    }
   ],
   "source": [
    "unw_rast = rio.open(unw)\n",
    "meta_data = unw_rast.profile\n",
    "print(meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-recognition",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
